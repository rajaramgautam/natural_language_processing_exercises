{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243bfd2e",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "# The end result of this exercise should be a file named prepare.py that defines the requested functions.\n",
    "\n",
    "# In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d377bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import acquire\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc367bc",
   "metadata": {},
   "source": [
    "# 1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "- Lowercase everything\n",
    "- Normalize unicode characters\n",
    "- Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d56796bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(article):\n",
    "    '''This function takes in string and converts to lower case, normalize unicode characters and\n",
    "        replace anything that is not a letter, number, whitespace or a single quote.\n",
    "    '''\n",
    "        \n",
    "    # lowercase everything\n",
    "    article = article.lower()\n",
    "    \n",
    "    # normalize unicode characters\n",
    "    article = unicodedata.normalize('NFKD', article)\\\n",
    "            .encode('ascii', 'ignore')\\\n",
    "            .decode('utf-8', 'ignore')\n",
    "    # Replace anything that is not a letter, number, whitespace or a single quote.\n",
    "    article = re.sub(r\"[^a-z0-9'\\s]\", '', article)\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd11a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"We are learning data science. We are investing time and money to learn new skills. \n",
    "We know that it will pay off. Manipulate, Manipulation, Data Science and Data Scientist\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d0e408a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we are learning data science we are investing time and money to learn new skills \\nwe know that it will pay off manipulate manipulation data science and data scientist'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling function\n",
    "basic_clean(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41559d50",
   "metadata": {},
   "source": [
    "# 2. Define a function named tokenize. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e451bfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(article):\n",
    "    \"\"\" This function takes in article parameter as sting and teturns a tokenized string. \"\"\"\n",
    "    # Creatr an object for tokenizer\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    \n",
    "    # Use tokenizer object to tokenize the parameter article here\n",
    "    article =  tokenizer.tokenize(article, return_str = True)\n",
    "    \n",
    "    return article\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "849f1f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We are learning data science. We are investing time and money to learn new skills. \\nWe know that it will pay off. Manipulate , Manipulation , Data Science and Data Scientist'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling function tokenize\n",
    "tokenize(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4728bc",
   "metadata": {},
   "source": [
    "# 3. Define a function named stem. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce90315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(article):\n",
    "    \n",
    "    \"\"\" This function takes in article parameter as string and returns article with words stemmed\"\"\"\n",
    "    \n",
    "    # Create the nltk stemmer object, then use it\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "    # Splitting the words in article parameter\n",
    "    article_splitted = article.split()\n",
    "    \n",
    "    # Use stemmer to stem each word in the list of words in article_splitted\n",
    "    stems = [ps.stem(word) for word in article_splitted]\n",
    "\n",
    "    # joining back all stemmed words stems here\n",
    "    article = ' '.join(stems)\n",
    "    \n",
    "    return article\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54172844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we are learn data science. we are invest time and money to learn new skills. we know that it will pay off. manipulate, manipulation, data scienc and data scientist'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling function stem\n",
    "stem(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2f9d88",
   "metadata": {},
   "source": [
    "# 4. Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e284aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(article):\n",
    "    \"\"\" This function will lemmatize the article parameter as string \n",
    "        and returns string with words lemmatized.\n",
    "    \"\"\"\n",
    "    # Create an object for lemmatizer\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # Splitting the words in article parameter\n",
    "    article_splitted = article.split()\n",
    "    \n",
    "    # Use lemmatizer object to lemmatize each word in the list of words in article_splitted\n",
    "    lemmas = [wnl.lemmatize(word) for word in article_splitted]\n",
    "\n",
    "    # joining back all stemmed words stems here\n",
    "    article = ' '.join(lemmas)\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dd299fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We are learning data science. We are investing time and money to learn new skills. We know that it will pay off. Manipulate, Manipulation, Data Science and Data Scientist'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9de1931",
   "metadata": {},
   "source": [
    "# 5. Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32807b8",
   "metadata": {},
   "source": [
    "# This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e523bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def remove_stopwords(article):\n",
    "    # stopword in english\n",
    "#    stopword_list = stopwords.words('english')\n",
    "    \n",
    "#    words = article.split()\n",
    "#    filtered_words = [w for w in words if w not in stopword_list]\n",
    "\n",
    "#    article_without_stopwords = ' '.join(filtered_words)\n",
    "\n",
    "#    return article_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c448eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_stopwords(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc751146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(article, extra_words = [], exclude_words = []):\n",
    "    \"\"\" This function takes in article as parameter as string, optional extra_words, \n",
    "    and optional exclude_words paramters with empty lists and returns a string.\"\"\"\n",
    "    # Create stopword_list object in english\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    # Removing exclude_words from stopword_list to keep these in my text.\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "    \n",
    "    # Adding extra_words to stopword_list to remove these in my text\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "    \n",
    "    # spliting words in article\n",
    "    words = article.split()\n",
    "    \n",
    "    # Creating a list of words from my article with stopwords removed and assign to variable\n",
    "    filtered_words = [w for w in words if w not in stopword_list]\n",
    "    \n",
    "    # Joining words in the filtered_words back into string and assign to variable\n",
    "    article_without_stopwords = ' '.join(filtered_words)\n",
    "\n",
    "    return article_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8799a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We learning data science. We investing time learn new skills. We know pay off. Manipulate, Manipulation, Data Science Data Scientist'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(article, extra_words = ['money'], exclude_words = ['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6160660",
   "metadata": {},
   "source": [
    "# 6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64002cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use get_inshorts_articles() from acquire.py file\n",
    "news_df = acquire.get_inshorts_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "259423b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I used to catch Patna-Banaras train to listen ...</td>\n",
       "      <td>Pragya Swastik</td>\n",
       "      <td>Vedanta's billionaire Chairman Anil Agarwal in...</td>\n",
       "      <td>06 Feb 2022,Sunday</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What can you say when you no longer have your ...</td>\n",
       "      <td>Kiran Khatri</td>\n",
       "      <td>After Lata Mangeshkar passed away at 92 on Sun...</td>\n",
       "      <td>06 Feb 2022,Sunday</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COVID, you did your worst &amp; stole our voice: M...</td>\n",
       "      <td>Sakshita Khosla</td>\n",
       "      <td>Businessman Anand Mahindra on Sunday shared a ...</td>\n",
       "      <td>06 Feb 2022,Sunday</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There's no greater tribute to our unity than L...</td>\n",
       "      <td>Hiral Goyal</td>\n",
       "      <td>Adani Group's Chairman Gautam Adani took to Tw...</td>\n",
       "      <td>06 Feb 2022,Sunday</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lata Mangeshkar will eternally cast her shadow...</td>\n",
       "      <td>Kiran Khatri</td>\n",
       "      <td>Biocon's Executive Chairperson Kiran Mazumdar-...</td>\n",
       "      <td>06 Feb 2022,Sunday</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title           author  \\\n",
       "0  I used to catch Patna-Banaras train to listen ...   Pragya Swastik   \n",
       "1  What can you say when you no longer have your ...     Kiran Khatri   \n",
       "2  COVID, you did your worst & stole our voice: M...  Sakshita Khosla   \n",
       "3  There's no greater tribute to our unity than L...      Hiral Goyal   \n",
       "4  Lata Mangeshkar will eternally cast her shadow...     Kiran Khatri   \n",
       "\n",
       "                                             content                date  \\\n",
       "0  Vedanta's billionaire Chairman Anil Agarwal in...  06 Feb 2022,Sunday   \n",
       "1  After Lata Mangeshkar passed away at 92 on Sun...  06 Feb 2022,Sunday   \n",
       "2  Businessman Anand Mahindra on Sunday shared a ...  06 Feb 2022,Sunday   \n",
       "3  Adani Group's Chairman Gautam Adani took to Tw...  06 Feb 2022,Sunday   \n",
       "4  Biocon's Executive Chairperson Kiran Mazumdar-...  06 Feb 2022,Sunday   \n",
       "\n",
       "   category  \n",
       "0  business  \n",
       "1  business  \n",
       "2  business  \n",
       "3  business  \n",
       "4  business  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65bd057",
   "metadata": {},
   "source": [
    "# 7. Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2faccdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df = acquire.get_blog_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0a0c09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>date published</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Codeup Dallas Open House</td>\n",
       "      <td>Come join us for the re-opening of our Dallas ...</td>\n",
       "      <td>Nov 30, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Codeup’s Placement Team Continues Setting Records</td>\n",
       "      <td>Our Placement Team is simply defined as a grou...</td>\n",
       "      <td>Nov 19, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT Certifications 101: Why They Matter, and Wh...</td>\n",
       "      <td>AWS, Google, Azure, Red Hat, CompTIA…these are...</td>\n",
       "      <td>Nov 18, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A rise in cyber attacks means opportunities fo...</td>\n",
       "      <td>In the last few months, the US has experienced...</td>\n",
       "      <td>Nov 17, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Use your GI Bill® benefits to Land a Job in Tech</td>\n",
       "      <td>As the end of military service gets closer, ma...</td>\n",
       "      <td>Nov 4, 2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                           Codeup Dallas Open House   \n",
       "1  Codeup’s Placement Team Continues Setting Records   \n",
       "2  IT Certifications 101: Why They Matter, and Wh...   \n",
       "3  A rise in cyber attacks means opportunities fo...   \n",
       "4   Use your GI Bill® benefits to Land a Job in Tech   \n",
       "\n",
       "                                             content date published  \n",
       "0  Come join us for the re-opening of our Dallas ...   Nov 30, 2021  \n",
       "1  Our Placement Team is simply defined as a grou...   Nov 19, 2021  \n",
       "2  AWS, Google, Azure, Red Hat, CompTIA…these are...   Nov 18, 2021  \n",
       "3  In the last few months, the US has experienced...   Nov 17, 2021  \n",
       "4  As the end of military service gets closer, ma...    Nov 4, 2021  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3061e43",
   "metadata": {},
   "source": [
    "# 8. For each dataframe, produce the following columns:\n",
    "\n",
    "- title to hold the title\n",
    "- original to hold the original article/post content\n",
    "- clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "- stemmed to hold the stemmed version of the cleaned data.\n",
    "- lemmatized to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ae2b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# title already hold title on both of them\n",
    "# replacing column content with 'original'\n",
    "news_df = news_df.rename(columns = {'content' : 'original'})\n",
    "\n",
    "# cleaning to hold the normalized and tokenized original with the stopwords removed\n",
    "codeup_df = codeup_df.rename(columns = {'content' : 'original'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "273706b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_article_data(df, column, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function take in a df and the string name for a text column with \n",
    "    option to pass lists for extra_words and exclude_words and\n",
    "    returns a df with the text article title, original text, stemmed text,\n",
    "    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\n",
    "    '''\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    df['stemmed'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(stem)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    df['lemmatized'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(lemmatize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    return df[['title', column,'clean', 'stemmed', 'lemmatized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "742382fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I used to catch Patna-Banaras train to listen ...</td>\n",
       "      <td>Vedanta's billionaire Chairman Anil Agarwal in...</td>\n",
       "      <td>vedanta ' billionaire chairman anil agarwal tw...</td>\n",
       "      <td>vedanta ' billionair chairman anil agarw tweet...</td>\n",
       "      <td>vedanta ' billionaire chairman anil agarwal tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What can you say when you no longer have your ...</td>\n",
       "      <td>After Lata Mangeshkar passed away at 92 on Sun...</td>\n",
       "      <td>lata mangeshkar passed away 92 sunday business...</td>\n",
       "      <td>lata mangeshkar pass away 92 sunday businessma...</td>\n",
       "      <td>lata mangeshkar passed away 92 sunday business...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COVID, you did your worst &amp; stole our voice: M...</td>\n",
       "      <td>Businessman Anand Mahindra on Sunday shared a ...</td>\n",
       "      <td>businessman anand mahindra sunday shared pictu...</td>\n",
       "      <td>businessman anand mahindra sunday share pictur...</td>\n",
       "      <td>businessman anand mahindra sunday shared pictu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There's no greater tribute to our unity than L...</td>\n",
       "      <td>Adani Group's Chairman Gautam Adani took to Tw...</td>\n",
       "      <td>adani group ' chairman gautam adani took twitt...</td>\n",
       "      <td>adani group ' chairman gautam adani took twitt...</td>\n",
       "      <td>adani group ' chairman gautam adani took twitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lata Mangeshkar will eternally cast her shadow...</td>\n",
       "      <td>Biocon's Executive Chairperson Kiran Mazumdar-...</td>\n",
       "      <td>biocon ' executive chairperson kiran mazumdars...</td>\n",
       "      <td>biocon ' execut chairperson kiran mazumdarshaw...</td>\n",
       "      <td>biocon ' executive chairperson kiran mazumdars...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  I used to catch Patna-Banaras train to listen ...   \n",
       "1  What can you say when you no longer have your ...   \n",
       "2  COVID, you did your worst & stole our voice: M...   \n",
       "3  There's no greater tribute to our unity than L...   \n",
       "4  Lata Mangeshkar will eternally cast her shadow...   \n",
       "\n",
       "                                            original  \\\n",
       "0  Vedanta's billionaire Chairman Anil Agarwal in...   \n",
       "1  After Lata Mangeshkar passed away at 92 on Sun...   \n",
       "2  Businessman Anand Mahindra on Sunday shared a ...   \n",
       "3  Adani Group's Chairman Gautam Adani took to Tw...   \n",
       "4  Biocon's Executive Chairperson Kiran Mazumdar-...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  vedanta ' billionaire chairman anil agarwal tw...   \n",
       "1  lata mangeshkar passed away 92 sunday business...   \n",
       "2  businessman anand mahindra sunday shared pictu...   \n",
       "3  adani group ' chairman gautam adani took twitt...   \n",
       "4  biocon ' executive chairperson kiran mazumdars...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  vedanta ' billionair chairman anil agarw tweet...   \n",
       "1  lata mangeshkar pass away 92 sunday businessma...   \n",
       "2  businessman anand mahindra sunday share pictur...   \n",
       "3  adani group ' chairman gautam adani took twitt...   \n",
       "4  biocon ' execut chairperson kiran mazumdarshaw...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  vedanta ' billionaire chairman anil agarwal tw...  \n",
       "1  lata mangeshkar passed away 92 sunday business...  \n",
       "2  businessman anand mahindra sunday shared pictu...  \n",
       "3  adani group ' chairman gautam adani took twitt...  \n",
       "4  biocon ' executive chairperson kiran mazumdars...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the function defined above for news_df's content column\n",
    "prep_article_data(news_df, 'original', extra_words=['ha'], exclude_words=['no']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1151775a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Codeup Dallas Open House</td>\n",
       "      <td>Come join us for the re-opening of our Dallas ...</td>\n",
       "      <td>come join us reopening dallas campus drinks sn...</td>\n",
       "      <td>come join us reopen dalla campu drink snack co...</td>\n",
       "      <td>come join u reopening dallas campus drink snac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Codeup’s Placement Team Continues Setting Records</td>\n",
       "      <td>Our Placement Team is simply defined as a grou...</td>\n",
       "      <td>placement team simply defined group manages re...</td>\n",
       "      <td>placement team simpli defin group manag relati...</td>\n",
       "      <td>placement team simply defined group manages re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT Certifications 101: Why They Matter, and Wh...</td>\n",
       "      <td>AWS, Google, Azure, Red Hat, CompTIA…these are...</td>\n",
       "      <td>aws google azure red hat comptiathese big name...</td>\n",
       "      <td>aw googl azur red hat comptiathes big name onl...</td>\n",
       "      <td>aws google azure red hat comptiathese big name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A rise in cyber attacks means opportunities fo...</td>\n",
       "      <td>In the last few months, the US has experienced...</td>\n",
       "      <td>last months us experienced dozens major cybera...</td>\n",
       "      <td>last month us experienc dozen major cyberattac...</td>\n",
       "      <td>last month u experienced dozen major cyberatta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Use your GI Bill® benefits to Land a Job in Tech</td>\n",
       "      <td>As the end of military service gets closer, ma...</td>\n",
       "      <td>end military service gets closer many transiti...</td>\n",
       "      <td>end militari servic get closer mani transit se...</td>\n",
       "      <td>end military service get closer many transitio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                           Codeup Dallas Open House   \n",
       "1  Codeup’s Placement Team Continues Setting Records   \n",
       "2  IT Certifications 101: Why They Matter, and Wh...   \n",
       "3  A rise in cyber attacks means opportunities fo...   \n",
       "4   Use your GI Bill® benefits to Land a Job in Tech   \n",
       "\n",
       "                                            original  \\\n",
       "0  Come join us for the re-opening of our Dallas ...   \n",
       "1  Our Placement Team is simply defined as a grou...   \n",
       "2  AWS, Google, Azure, Red Hat, CompTIA…these are...   \n",
       "3  In the last few months, the US has experienced...   \n",
       "4  As the end of military service gets closer, ma...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  come join us reopening dallas campus drinks sn...   \n",
       "1  placement team simply defined group manages re...   \n",
       "2  aws google azure red hat comptiathese big name...   \n",
       "3  last months us experienced dozens major cybera...   \n",
       "4  end military service gets closer many transiti...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  come join us reopen dalla campu drink snack co...   \n",
       "1  placement team simpli defin group manag relati...   \n",
       "2  aw googl azur red hat comptiathes big name onl...   \n",
       "3  last month us experienc dozen major cyberattac...   \n",
       "4  end militari servic get closer mani transit se...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  come join u reopening dallas campus drink snac...  \n",
       "1  placement team simply defined group manages re...  \n",
       "2  aws google azure red hat comptiathese big name...  \n",
       "3  last month u experienced dozen major cyberatta...  \n",
       "4  end military service get closer many transitio...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the function defined above for news_df's content column\n",
    "prep_article_data(codeup_df, 'original', extra_words=['ha'], exclude_words=['no']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc09960",
   "metadata": {},
   "source": [
    "# 9. Ask yourself:\n",
    "\n",
    "- If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "- If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "- If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da4ccb6",
   "metadata": {},
   "source": [
    "### For 200TB one stemmed text but for 493KB and 25MB, I would go with lemmatized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5d8d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
